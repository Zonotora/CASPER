{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\Anaconda3\\lib\\site-packages\\pandas\\compat\\_optional.py:138: UserWarning: Pandas requires version '2.7.0' or newer of 'numexpr' (version '2.6.9' currently installed).\n",
      "  warnings.warn(msg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import ipinfo\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "import requests\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from geopy.geocoders import Nominatim\n",
    "import csv\n",
    "import multiprocessing\n",
    "import json\n",
    "import pycountry\n",
    "import country_converter as coco  #Coco was found to be more accuracte. Pycountry had weird labels for e.g. russia\n",
    "\n",
    "\n",
    "def get_ip():\n",
    "    response = requests.get('https://api64.ipify.org?format=json').json()\n",
    "    return response[\"ip\"]\n",
    "\n",
    "\n",
    "def get_location(ip):\n",
    "    response = requests.get(f'https://ipapi.co/{ip}/json/').json()\n",
    "    location_data = {\n",
    "        \"ip\": ip,\n",
    "        \"city\": response.get(\"city\"),\n",
    "        \"region\": response.get(\"region\"),\n",
    "        \"country\": response.get(\"country_name\")\n",
    "    }\n",
    "    return location_data\n",
    "\n",
    "def get_location_ipinfo(ip_address):\n",
    "    ''' Best one of the three. However, IP requests is limited to the token\n",
    "    '''\n",
    "    try:\n",
    "        access_token = '7dbb53d0419093'\n",
    "        handler = ipinfo.getHandler(access_token)\n",
    "        details = handler.getDetails(ip_address)\n",
    "        if details.country == \"US\": \n",
    "            return details.region\n",
    "        else:\n",
    "            return details.country\n",
    "    except:\n",
    "        print(\"error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_ip_locations(df):\n",
    "    df[\"MinIP_loc\"] = np.nan\n",
    "    df[\"otherIP1_loc\"] = np.nan \n",
    "    df[\"otherIP2_loc\"] = np.nan \n",
    "    df[\"otherIP3_loc\"] = np.nan \n",
    "    df[\"otherIP4_loc\"] = np.nan \n",
    "    df[\"otherIP5_loc\"] = np.nan \n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        df.loc[index, \"MinIP_loc\"] = get_location_ipinfo(row[\"MinIP\"])\n",
    "        for i in range(1,6):  \n",
    "            name = f\"otherIP{i}\"\n",
    "            if pd.isna(row[name]):\n",
    "                continue\n",
    "\n",
    "            location = get_location_ipinfo(row[f\"otherIP{i}\"])\n",
    "            df.loc[index, f\"otherIP{i}_loc\"] = location\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prb_location(df_prbs, prb_dic):\n",
    "    '''Returns a dataframe with prb number and their respective country or state \n",
    "    '''\n",
    "    geolocator = Nominatim(user_agent=\"geoapiExercises\")\n",
    "    df_prbs[\"prb_loc\"] = \"\"\n",
    "    for index, row in df_prbs.iterrows():\n",
    "        prb_number = row[\"prb\"]\n",
    "        long,lat = tuple(prb_dic[prb_number][\"geometry\"][\"coordinates\"])\n",
    "        location = \"\"\n",
    "        try:\n",
    "            location = geolocator.reverse(str(lat) + \",\" + str(long), language = 'en').raw\n",
    "        except: \n",
    "            print(\"timeout at index: \" + str(index))\n",
    "            continue\n",
    "        country = location[\"address\"][\"country\"]\n",
    "        \n",
    "        if country == \"Canada\":\n",
    "            state = location[\"address\"][\"state\"]\n",
    "            df_prbs.loc[index, \"prb_loc\"] = state\n",
    "        elif country == \"United States\":\n",
    "            state = str((location)).split(\",\")[-3]\n",
    "            state = location[\"address\"][\"state\"]\n",
    "            df_prbs.loc[index, \"prb_loc\"] = state\n",
    "        else:\n",
    "            country_code = location[\"address\"][\"country_code\"].upper() \n",
    "            df_prbs.loc[index, \"prb_loc\"] = country_code\n",
    "        if index%1000 == 0:\n",
    "            print(\"AT INDEX:   \" + str(index))\n",
    "    return df_prbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_prb_locs(df_edges,pickle_path):\n",
    "\n",
    "    df_prbs_np_arr = df_edges[\"prb\"].unique()\n",
    "    df_prbs = pd.DataFrame(df_prbs_np_arr, columns=[\"prb\"])\n",
    "\n",
    "    with open(pickle_path, 'rb') as handle:\n",
    "        prb_dic = pickle.load(handle)\n",
    "        df_prb_loc = prb_location(df_prbs, prb_dic)\n",
    "        df_prbs = df_prb_loc\n",
    "        #df_prb_loc.to_csv(r\"api\\latencies\\prbs_with_locations.csv\", index = False)\n",
    "    #return df_prbs\n",
    "    return df_prb_loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_prb_locs_with_ip_locs(df_prb_loc, df_with_ip_locs):\n",
    "    ''' Takes the csv with prbs and their respecitve locations to merge with the file containing ip locations\n",
    "    ''' \n",
    "    prb_to_loc = {}\n",
    "    for index, row in df_prb_loc.iterrows():\n",
    "        prb_to_loc[row[\"prb\"]] = row[\"prb_loc\"]\n",
    "    \n",
    "    df_with_ip_locs[\"prb_loc\"] = \"\"\n",
    "    for index, row in df_with_ip_locs.iterrows(): \n",
    "        df_with_ip_locs.loc[index,\"prb_loc\"] = prb_to_loc[row[\"prb\"]]\n",
    "\n",
    "    return df_with_ip_locs\n",
    "\n",
    "def reorder_columns_of_final_df(merged_df):\n",
    "    ''' Reformats the file by column order\n",
    "    '''\n",
    "    cols = [\n",
    "        \"prb\" , \"prb_loc\",\n",
    "        \"MinIP\", \"MinIP_loc\", \"Minlatency\",\n",
    "        \"otherIP1\", \"otherIP1_loc\", \"otherlatency1\",\n",
    "        \"otherIP2\", \"otherIP2_loc\", \"otherlatency2\",\n",
    "        \"otherIP3\", \"otherIP3_loc\", \"otherlatency3\",\n",
    "        \"otherIP4\", \"otherIP4_loc\", \"otherlatency4\",\n",
    "        \"otherIP5\", \"otherIP5_loc\", \"otherlatency5\",\n",
    "        ]\n",
    "\n",
    "    merged_df = merged_df[cols]\n",
    "    \n",
    "    merged_df = merged_df.rename({\n",
    "        'otherIP1': 'IP1', \n",
    "        'otherIP2': 'IP2', \n",
    "        'otherIP3': 'IP3', \n",
    "        'otherIP4': 'IP4', \n",
    "        'otherIP5': 'IP5',\n",
    "        'otherIP1_loc': 'IP1_loc', \n",
    "        'otherIP2_loc': 'IP2_loc', \n",
    "        'otherIP3_loc': 'IP3_loc', \n",
    "        'otherIP4_loc': 'IP4_loc', \n",
    "        'otherIP5_loc': 'IP5_loc',\n",
    "        'otherlatency1': 'latency1', \n",
    "        'otherlatency2': 'latency2',\n",
    "        'otherlatency3': 'latency3',\n",
    "        'otherlatency4': 'latency4',\n",
    "        'otherlatency5': 'latency5'\n",
    "        }, axis='columns')\n",
    "\n",
    "    return merged_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_country_code(df):\n",
    "    ''' Converts non-'North American' countries to country code\n",
    "    '''\n",
    "    cc = coco.CountryConverter()\n",
    "    #df = df.apply(lambda x: pycountry.countries.get(name=str(x)) if pycountry.countries.get(name=str(x)) != None else print(x))\n",
    "    df = df.apply(lambda x: coco.convert(names=x, to='ISO2', not_found = x) if coco.convert(names=x, to='ISO2', not_found = None) != None else x)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_adj_mtx(edge_path, locations):\n",
    "    '''Creates an adjacency matrix and returns it to later be saved with picke\n",
    "    '''\n",
    "    df_adj = pd.DataFrame(columns=locations, index=locations, dtype = object)\n",
    "\n",
    "    df_edges = pd.read_csv(edge_path, index_col=False)\n",
    "    cols = [\"IP1_loc\", \"IP2_loc\", \"IP3_loc\", \"IP4_loc\", \"IP5_loc\", \"MinIP_loc\"]\n",
    "    \n",
    "    for index, row in df_edges.iterrows():\n",
    "        from_loc = row[\"prb_loc\"]\n",
    "        # skip if datacenter's location not sought after\n",
    "        if from_loc not in locations:\n",
    "            continue\n",
    "        for col in cols: \n",
    "            to_loc = row[col]     \n",
    "            # if user's request location not sought after  \n",
    "            if to_loc not in locations:\n",
    "                continue  \n",
    "            col_idx = df_edges.columns.get_loc(col)\n",
    "            latency = row.iloc[col_idx+1]    \n",
    "            saved_latencies = df_adj.loc[from_loc,to_loc]\n",
    "            if np.isnan(saved_latencies).all():\n",
    "                df_adj.loc[from_loc,to_loc] = list([float(latency)])\n",
    "                df_adj.loc[to_loc,from_loc] = list([float(latency)])\n",
    "            else:\n",
    "                df_adj.loc[from_loc,to_loc].append(float(latency))\n",
    "                df_adj.loc[to_loc, from_loc].append(float(latency))\n",
    "    return df_adj\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_regions(df):\n",
    "    ''' Returns all mentioned locations in edge latency file\n",
    "    '''\n",
    "    df_locs = pd.concat([df[\"MinIP_loc\"], df[\"IP1_loc\"], df[\"IP2_loc\"], df[\"IP3_loc\"], df[\"IP4_loc\"], df[\"IP5_loc\"]])\n",
    "    return df_locs.unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def no_empty_cells(df):\n",
    "    no = 0\n",
    "    for index, row in df.iterrows():\n",
    "        no += row.isna().sum()\n",
    "    return no\n",
    "\n",
    "def no_all_latencies(df):\n",
    "    no = 0\n",
    "    for index, row in df.iterrows():\n",
    "        no += row.notna().count()\n",
    "    return no\n",
    "\n",
    "def all_latencies_dict(df):\n",
    "    no = defaultdict(lambda:0)\n",
    "    for index, row in df.iterrows():\n",
    "        print(row.isna())\n",
    "        no[index] += row.isna().count()\n",
    "    return no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prb_loc(prb_id):\n",
    "    geolocator = Nominatim(user_agent=\"geoapiExercises\")\n",
    "    response = requests.get(f\"https://atlas.ripe.net/api/v2/probes/{prb_id}/?format=json\")\n",
    "    resp_json = json.loads(response.content.decode(\"utf-8\"))\n",
    "    long, lat = tuple(resp_json[\"geometry\"][\"coordinates\"])\n",
    "    \n",
    "    location = geolocator.reverse(str(lat) + \",\" + str(long), language = 'en')\n",
    "\n",
    "    country = location.raw[\"address\"][\"country\"]\n",
    "\n",
    "    if country is \"United States\" or \"Canada\":\n",
    "        state = location.raw[\"address\"][\"state\"]\n",
    "        return state\n",
    "    else: \n",
    "        country_code = location[\"address\"][\"country_code\"].upper() \n",
    "        return country_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Poland\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'country' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-d05690a77de5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mget_prb_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m50050\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-19-159ee3db8993>\u001b[0m in \u001b[0;36mget_prb_loc\u001b[1;34m(prb_id)\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mcountry\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'country' is not defined"
     ]
    }
   ],
   "source": [
    "get_prb_loc(50050)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119\n"
     ]
    }
   ],
   "source": [
    "### This extracts probe id and maps it to a location\n",
    "\n",
    "# df_edges = pd.read_csv(r\"api\\latencies\\edge.csv\", index_col= False)\n",
    "# df_prb_locs = extract_prb_locs(df_edges, picke_path = r'api\\latencies\\probes_clean.pickle')\n",
    "# df_prb_locs.to_csv(r\"api\\latencies\\prbs_with_locations.csv\", index = False)\n",
    "\n",
    "\n",
    "# timeout at index: 7775\n",
    "# AT INDEX:   8000\n",
    "# timeout at index: 8215\n",
    "\n",
    "### This assign a country code to non us/canada countries. they use states instead\n",
    "\n",
    "#df_prbs = pd.read_csv(r\"api\\latencies\\prbs_with_locations.csv\", index_col= False)\n",
    "#df_prbs[\"prb_loc\"] = convert_country_code(df_prbs[\"prb_loc\"])\n",
    "#df_prbs.to_csv(r\"api\\latencies\\prbs_with_locations_coco.csv\", index = False)\n",
    "\n",
    "### THIS adds location to the fields [MinIP, OtherIP{1,2,3,4,5}]\n",
    "\n",
    "#df_with_ip_locs = add_ip_locations(df_edges)\n",
    "#df_with_ip_locs.to_csv(r\"api\\latencies\\edge_with_ip_locs.csv\", index= False)\n",
    "\n",
    "\n",
    "### BELOW Is to merge files and formatting\n",
    "\n",
    "# df_prb_loc = pd.read_csv(r\"api\\latencies\\prbs_with_locations.csv\", index_col = False)\n",
    "# df_with_ip_locs = pd.read_csv(r\"api\\latencies\\edge_processed10.csv\", index_col = False)\n",
    "\n",
    "# df_merged = merge_prb_locs_with_ip_locs(df_prb_loc, df_with_ip_locs)\n",
    "\n",
    "# df_merged_formated = reorder_columns_of_final_df(df_merged)\n",
    "# df_merged_formated.to_csv(r\"api\\latencies\\edge_feat_locations.csv\", index = False)\n",
    "\n",
    "### Creates adjacency matrix out of the edge latencies\n",
    "\n",
    "# df = pd.read_csv(r\"..\\api\\latencies\\edge_feat_locations.csv\", index_col=False)\n",
    "# regions = get_all_regions(df)\n",
    "# # Filter out NaN and regions\n",
    "# na = [region for region in regions if type(region) == str and len(region) > 2]\n",
    "# eu = [region for region in regions if type(region) == str and len(region) < 3]\n",
    "# na.remove(\"Oklahoma\")\n",
    "# na.remove(\"Alabama\")\n",
    "# na.remove(\"Mississippi\")\n",
    "# na.remove(\"South Dakota\")\n",
    "# na.remove(\"Nebraska\")\n",
    "# na.remove(\"Delaware\")\n",
    "# na.remove(\"Montana\")\n",
    "# na.remove(\"Alaska\")\n",
    "#na = [region for region in na if region in [\"California\", \"Massachusetts\", \"Arizona\"]]\n",
    "# # Is this number correct? \n",
    "# print(\"Number of regions in NA: \" + str(len(na)))\n",
    "# print(\"N.o. all latencies: \" + str(no_all_latencies(df)))\n",
    "# mtrx_df = create_adj_mtx(r\"..\\api\\latencies\\edge_feat_locations.csv\", na)\n",
    "# mtrx_df.to_pickle(r\"..\\api\\latencies\\adjacency_mtrx.pickle\")\n",
    "# print(all_latencies_dict(mtrx_df))\n",
    "# print(\"N.o. missing datapoints: \" + str(no_empty_cells(mtrx_df)) + \" out of \" + str(len(na)*len(na)))\n",
    "\n",
    "# mtrx_df.to_csv(r\"..\\api\\latencies\\edge_feat_locations_na.csv\")\n",
    "\n",
    "### Add locations to cloud_data\n",
    "\n",
    "cloud_df = pd.read_csv(r\"..\\api\\latencies\\cloud.csv\")\n",
    "edge_df = pd.read_csv(r\"..\\api\\latencies\\edge_feat_locations.csv\")\n",
    "cloud_with_locs = merge_locs(edge_df, cloud_df)\n",
    "cloud_with_locs.to_csv(r\"..\\api\\latencies\\cloud_feat_locations.csv\", index= False)\n",
    "\n",
    "print(\"This many prb_locs couldn't resolved by merge: \" + check_empty(cloud_with_locs))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@DeprecationWarning\n",
    "def prb_location_old(df_edges):\n",
    "    geolocator = Nominatim(user_agent=\"geoapiExercises\")\n",
    "    df_edges[\"prb_loc\"] = \"\"\n",
    "    for index, row in df_edges.iterrows():\n",
    "        prb_number = row[\"prb\"]\n",
    "        response = requests.get(f\"https://atlas.ripe.net/api/v2/probes/{prb_number}/?format=json\")\n",
    "        resp_json = json.loads(response.content.decode(\"utf-8\"))\n",
    "        long, lat = tuple(resp_json[\"geometry\"][\"coordinates\"])\n",
    "        \n",
    "        location = geolocator.reverse(str(lat) + \",\" + str(long), language = 'en')\n",
    "        country = str((location)).split(\",\")[-1]\n",
    "        \n",
    "        if location == \"United States\":\n",
    "            state = str((location)).split(\",\")[-3]\n",
    "            df_edges.loc[index, \"prb_loc\"] = state\n",
    "        else: \n",
    "            df_edges.loc[index, \"prb_loc\"] = country\n",
    "    return df_edges\n",
    "\n",
    "    \n",
    "@DeprecationWarning\n",
    "def intersects_of_prbs_to_csv():\n",
    "\n",
    "    df = pd.read_csv(r\"C:\\Users\\Admin\\Documents\\GitHub\\umass\\api\\latencies\\edge.csv\", index_col= False)\n",
    "    dfs = [df[df[\"prb\"].isin(dic[region])] for region in dic.keys()]\n",
    "    for df in dfs: \n",
    "        for index, row in df.iterrows():\n",
    "            df[\"MinIP_loc\"] = get_location_ipinfo(row[\"MinIP\"])\n",
    "            for i in range(1,6):  \n",
    "                name = f\"otherIP{i}\"\n",
    "                if pd.isna(row[name]):\n",
    "                    continue\n",
    "                location_data = get_location_ipinfo(row[f\"otherIP{i}\"])\n",
    "                df[f\"otherIP{i}_loc\"] = location_data\n",
    "    concated = pd.concat([df for df in dfs], ignore_index=True)\n",
    "    concated.to_csv(r\"C:\\Users\\Admin\\Documents\\GitHub\\umass\\api\\latencies\\edge_processed4.csv\", index= False)\n",
    "\n",
    "@DeprecationWarning\n",
    "def singleprocess_prb_location():\n",
    "    df_edges = pd.read_csv(r\"api\\latencies\\edge.csv\", index_col= False)\n",
    "    df_prbs_np_arr = df_edges[\"prb\"].unique()\n",
    "    df_prbs = pd.DataFrame(df_prbs_np_arr, columns=[\"prb\"])\n",
    "    df_prbs = prb_location(df_prbs)\n",
    "    df_prbs.to_csv(r\"api\\latencies\\edge_processed_testing2.csv\", index = False)\n",
    "\n",
    "@DeprecationWarning\n",
    "def multiprocess_prb_location():\n",
    "    df_edges = pd.read_csv(r\"api\\latencies\\edge.csv\", index_col= False)\n",
    "    df_prbs_np_arr = df_edges[\"prb\"].unique()\n",
    "    df_prbs = pd.DataFrame(df_prbs_np_arr, columns=[\"prb\"])\n",
    "    num_processes = multiprocessing.cpu_count() - 1\n",
    "    chunk_size = int(df_prbs.shape[0]/num_processes)\n",
    "    chunks = [df_prbs.iloc[df_prbs.index[i:i + chunk_size]] for i in range(0, df_prbs.shape[0], chunk_size)]\n",
    "    df_prbs = prb_location(df_prbs)\n",
    "\n",
    "    pool = multiprocessing.Pool(processes=num_processes)\n",
    "    result = pool.map(prb_location, chunks)\n",
    "\n",
    "    for i in range(len(result)):\n",
    "    # we can reassign the original dataframe based on the index of each chunk\n",
    "        df_prbs.iloc[result[i].index] = result[i]\n",
    "\n",
    "    df_prbs.to_csv(r\"api\\latencies\\edge_processed_testing2.csv\", index = False)\n",
    "\n",
    "@DeprecationWarning\n",
    "def create_adj_matrix(df):\n",
    "    regions_set = set()\n",
    "    cols = [\"otherIP1_loc\", \"otherIP2_loc\", \"otherIP3_loc\", \"otherIP4_loc\", \"otherIP5_loc\", \"MinIP_loc\"]\n",
    "    for index, row in df.iterrows():\n",
    "        for col in cols:\n",
    "            regions_set.add(str(row[col]))\n",
    "    regions_set.remove(\"None\")\n",
    "    regions_set.remove(\"nan\")\n",
    "\n",
    "    adj = pd.DataFrame(columns = list(regions_set))\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        for col in cols: \n",
    "            lat_col = col.replace(\"_loc\", \"\")\n",
    "            lat_col = lat_col.replace(\"IP\", \"latency\")\n",
    "            region_from = row[col]\n",
    "            region_to = row[\"prb_loc\"]\n",
    "            region_from_idx = adj.columns.get_loc(region_from)\n",
    "            region_to_idx = adj.columns.get_loc(region_to)\n",
    "            \n",
    "            latency = row.loc[lat_col]\n",
    "            adj.iloc[region_from_idx, region_to_idx] = latency\n",
    "            adj.iloc[region_to] = latency\n",
    "    #adj.to_csv(r\"api\\latencies\\adjance_latency.csv\", index=False)\n",
    "    return adj"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.3 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fe6f22945cfc871c9399bf7d9f19a00d7d184a1ba9bcdcec8f1d4899546bde82"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
